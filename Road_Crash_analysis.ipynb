{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A : Working with RDDs and DataFrames "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Preparation and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "master = \"local[*]\"\n",
    "# Naming The `appName` field, i.e., is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment 1\"\n",
    "\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Importing SparkSession classes \n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Initialize Spark Session and create a SparkContext Object\n",
    "from pyspark import SparkContext # Spark\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Import all the csv files from 2015-2019 into a single RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all Units files to rdd using wildcard \"*\"\n",
    "units_rdd = sc.textFile('*Units.csv')\n",
    "\n",
    "# Importing all Crash files to rdd using wildcard \"*\"\n",
    "crash_rdd = sc.textFile('*Crash.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Use csv.reader to parse each row as a list in the RDD, and remove the header rows and display the total count and first 10 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Total count of units : 153854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['\"2016-1-15/08/2019\"',\n",
       "  '\"01\"',\n",
       "  '0',\n",
       "  '\"SA\"',\n",
       "  '\"OMNIBUS\"',\n",
       "  '\"2011\"',\n",
       "  '\"North\"',\n",
       "  '\"Male\"',\n",
       "  '\"056\"',\n",
       "  '\"SA\"',\n",
       "  '\"HR\"',\n",
       "  '\"Full\"',\n",
       "  '\"Not Towing\"',\n",
       "  '\"Straight Ahead\"',\n",
       "  '\"010\"',\n",
       "  '\"5121\"',\n",
       "  '',\n",
       "  ''],\n",
       " ['\"2016-1-15/08/2019\"',\n",
       "  '\"02\"',\n",
       "  '1',\n",
       "  '',\n",
       "  '\"Pedestrian on Road\"',\n",
       "  '',\n",
       "  '\"East\"',\n",
       "  '\"Male\"',\n",
       "  '\"072\"',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '\"Walking on Road\"',\n",
       "  '',\n",
       "  '\"5084\"',\n",
       "  '',\n",
       "  ''],\n",
       " ['\"2016-2-15/08/2019\"',\n",
       "  '\"01\"',\n",
       "  '0',\n",
       "  '\"SA\"',\n",
       "  '\"Motor Cars - Sedan\"',\n",
       "  '\"2004\"',\n",
       "  '\"Unknown\"',\n",
       "  '\"Female\"',\n",
       "  '\"023\"',\n",
       "  '\"SA\"',\n",
       "  '\"C \"',\n",
       "  '\"Full\"',\n",
       "  '\"Not Towing\"',\n",
       "  '\"Straight Ahead\"',\n",
       "  '\"001\"',\n",
       "  '\"5087\"',\n",
       "  '',\n",
       "  ''],\n",
       " ['\"2016-2-15/08/2019\"',\n",
       "  '\"02\"',\n",
       "  '0',\n",
       "  '\"SA\"',\n",
       "  '\"Station Wagon\"',\n",
       "  '\"2008\"',\n",
       "  '\"Unknown\"',\n",
       "  '\"Male\"',\n",
       "  '\"040\"',\n",
       "  '\"SA\"',\n",
       "  '\"C \"',\n",
       "  '\"Full\"',\n",
       "  '\"Not Towing\"',\n",
       "  '\"Straight Ahead\"',\n",
       "  '\"001\"',\n",
       "  '\"5084\"',\n",
       "  '',\n",
       "  ''],\n",
       " ['\"2016-3-15/08/2019\"',\n",
       "  '\"01\"',\n",
       "  '0',\n",
       "  '\"SA\"',\n",
       "  '\"RIGID TRUCK LGE GE 4.5T\"',\n",
       "  '\"1990\"',\n",
       "  '\"South\"',\n",
       "  '\"Unknown\"',\n",
       "  '\"XXX\"',\n",
       "  '\"SA\"',\n",
       "  '\"MR\"',\n",
       "  '\"Provisional 2\"',\n",
       "  '\"Not Towing\"',\n",
       "  '\"Straight Ahead\"',\n",
       "  '\"001\"',\n",
       "  '\"5115\"',\n",
       "  '',\n",
       "  ''],\n",
       " ['\"2016-3-15/08/2019\"',\n",
       "  '\"02\"',\n",
       "  '0',\n",
       "  '\"SA\"',\n",
       "  '\"Panel Van\"',\n",
       "  '\"2013\"',\n",
       "  '\"South\"',\n",
       "  '\"Male\"',\n",
       "  '\"023\"',\n",
       "  '\"SA\"',\n",
       "  '\"C \"',\n",
       "  '\"Full\"',\n",
       "  '\"Not Towing\"',\n",
       "  '\"Straight Ahead\"',\n",
       "  '\"001\"',\n",
       "  '\"5110\"',\n",
       "  '',\n",
       "  ''],\n",
       " ['\"2016-4-15/08/2019\"',\n",
       "  '\"01\"',\n",
       "  '0',\n",
       "  '\"SA\"',\n",
       "  '\"Station Wagon\"',\n",
       "  '\"2002\"',\n",
       "  '\"East\"',\n",
       "  '\"Female\"',\n",
       "  '\"033\"',\n",
       "  '\"SA\"',\n",
       "  '\"C \"',\n",
       "  '\"Full\"',\n",
       "  '\"Not Towing\"',\n",
       "  '\"Straight Ahead\"',\n",
       "  '\"001\"',\n",
       "  '\"5169\"',\n",
       "  '',\n",
       "  ''],\n",
       " ['\"2016-4-15/08/2019\"',\n",
       "  '\"02\"',\n",
       "  '0',\n",
       "  '\"UNKNOWN\"',\n",
       "  '\"Other Defined Special Vehicle\"',\n",
       "  '\"XXXX\"',\n",
       "  '\"North\"',\n",
       "  '\"Unknown\"',\n",
       "  '\"XXX\"',\n",
       "  '\"UNKNOWN\"',\n",
       "  '\"XX\"',\n",
       "  '\"Unknown\"',\n",
       "  '\"Unknown\"',\n",
       "  '\"Reversing\"',\n",
       "  '\"001\"',\n",
       "  '\"XXXX\"',\n",
       "  '',\n",
       "  ''],\n",
       " ['\"2016-5-15/08/2019\"',\n",
       "  '\"01\"',\n",
       "  '1',\n",
       "  '\"SA\"',\n",
       "  '\"Motor Cars - Sedan\"',\n",
       "  '\"1997\"',\n",
       "  '\"South East\"',\n",
       "  '\"Male\"',\n",
       "  '\"042\"',\n",
       "  '\"SA\"',\n",
       "  '\"C \"',\n",
       "  '\"Full\"',\n",
       "  '\"Not Towing\"',\n",
       "  '\"Right Turn\"',\n",
       "  '\"001\"',\n",
       "  '\"XXXX\"',\n",
       "  '',\n",
       "  ''],\n",
       " ['\"2016-5-15/08/2019\"',\n",
       "  '\"02\"',\n",
       "  '2',\n",
       "  '\"SA\"',\n",
       "  '\"Utility\"',\n",
       "  '\"2015\"',\n",
       "  '\"North East\"',\n",
       "  '\"Male\"',\n",
       "  '\"059\"',\n",
       "  '\"SA\"',\n",
       "  '\"MC\"',\n",
       "  '\"Full\"',\n",
       "  '\"Not Towing\"',\n",
       "  '\"Straight Ahead\"',\n",
       "  '\"002\"',\n",
       "  '\"5114\"',\n",
       "  '',\n",
       "  '']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Units RDD\n",
    "\n",
    "# Split each line separated by comma into a list \n",
    "unitsrdd = units_rdd.map(lambda line: line.split(','))\n",
    "# Remove the header\n",
    "header = unitsrdd.first()\n",
    "units_final = unitsrdd.filter(lambda row: row != header)   #filter out header\n",
    "# Print how many final records\n",
    "print(f\"\\n Total count of units : {units_final.count()}\")\n",
    "units_final.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Total count of Crashes : 72006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['\"2019-1-8/07/2020\"',\n",
       "  '\"2 Metropolitan\"',\n",
       "  '\"HAMPSTEAD GARDENS\"',\n",
       "  '\"5086\"',\n",
       "  '\"CITY OF PORT ADELAIDE ENFIELD\"',\n",
       "  '2',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '2019',\n",
       "  '\"June\"',\n",
       "  '\"Wednesday\"',\n",
       "  '\"11:15 am\"',\n",
       "  '\"060\"',\n",
       "  '\"Cross Road\"',\n",
       "  '\"Straight road\"',\n",
       "  '\"Level\"',\n",
       "  '\"Not Applicable\"',\n",
       "  '\"Sealed\"',\n",
       "  '\"Dry\"',\n",
       "  '\"Not Raining\"',\n",
       "  '\"Daylight\"',\n",
       "  '\"Right Angle\"',\n",
       "  '\"01\"',\n",
       "  '\"Driver Rider\"',\n",
       "  '\"1: PDO\"',\n",
       "  '\"Give Way Sign\"',\n",
       "  '\"\"',\n",
       "  '\"\"',\n",
       "  '1331810.03',\n",
       "  '1676603.26',\n",
       "  '\"13318101676603\"'],\n",
       " ['\"2019-2-8/07/2020\"',\n",
       "  '\"2 Metropolitan\"',\n",
       "  '\"DRY CREEK\"',\n",
       "  '\"5094\"',\n",
       "  '\"CITY OF SALISBURY\"',\n",
       "  '2',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '2019',\n",
       "  '\"January\"',\n",
       "  '\"Tuesday\"',\n",
       "  '\"12:49 am\"',\n",
       "  '\"090\"',\n",
       "  '\"Divided Road\"',\n",
       "  '\"Straight road\"',\n",
       "  '\"Level\"',\n",
       "  '\"Not Applicable\"',\n",
       "  '\"Sealed\"',\n",
       "  '\"Dry\"',\n",
       "  '\"Not Raining\"',\n",
       "  '\"Night\"',\n",
       "  '\"Rear End\"',\n",
       "  '\"02\"',\n",
       "  '\"Driver Rider\"',\n",
       "  '\"1: PDO\"',\n",
       "  '\"No Control\"',\n",
       "  '\"\"',\n",
       "  '\"\"',\n",
       "  '1328376.2',\n",
       "  '1682942.63',\n",
       "  '\"13283761682943\"'],\n",
       " ['\"2019-3-8/07/2020\"',\n",
       "  '\"2 Metropolitan\"',\n",
       "  '\"MILE END\"',\n",
       "  '\"5031\"',\n",
       "  '\"CITY OF WEST TORRENS\"',\n",
       "  '2',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2019',\n",
       "  '\"January\"',\n",
       "  '\"Tuesday\"',\n",
       "  '\"12:00 am\"',\n",
       "  '\"060\"',\n",
       "  '\"Divided Road\"',\n",
       "  '\"Straight road\"',\n",
       "  '\"Level\"',\n",
       "  '\"Not Applicable\"',\n",
       "  '\"Sealed\"',\n",
       "  '\"Dry\"',\n",
       "  '\"Not Raining\"',\n",
       "  '\"Night\"',\n",
       "  '\"Hit Pedestrian\"',\n",
       "  '\"01\"',\n",
       "  '\"Driver Rider\"',\n",
       "  '\"2: MI\"',\n",
       "  '\"No Control\"',\n",
       "  '\"\"',\n",
       "  '\"\"',\n",
       "  '1325819.68',\n",
       "  '1670994.26',\n",
       "  '\"13258201670994\"'],\n",
       " ['\"2019-4-8/07/2020\"',\n",
       "  '\"2 Metropolitan\"',\n",
       "  '\"PARALOWIE\"',\n",
       "  '\"5108\"',\n",
       "  '\"CITY OF SALISBURY\"',\n",
       "  '2',\n",
       "  '1',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '2019',\n",
       "  '\"January\"',\n",
       "  '\"Tuesday\"',\n",
       "  '\"12:05 am\"',\n",
       "  '\"050\"',\n",
       "  '\"Not Divided\"',\n",
       "  '\"CURVED',\n",
       "  ' VIEW OPEN\"',\n",
       "  '\"Level\"',\n",
       "  '\"Not Applicable\"',\n",
       "  '\"Sealed\"',\n",
       "  '\"Dry\"',\n",
       "  '\"Not Raining\"',\n",
       "  '\"Night\"',\n",
       "  '\"Hit Fixed Object\"',\n",
       "  '\"01\"',\n",
       "  '\"Driver Rider\"',\n",
       "  '\"3: SI\"',\n",
       "  '\"No Control\"',\n",
       "  '\"\"',\n",
       "  '\"\"',\n",
       "  '1328320.6',\n",
       "  '1690237.08',\n",
       "  '\"13283211690237\"'],\n",
       " ['\"2019-5-8/07/2020\"',\n",
       "  '\"2 Metropolitan\"',\n",
       "  '\"MOUNT BARKER\"',\n",
       "  '\"5251\"',\n",
       "  '\"DC MT.BARKER.                 \"',\n",
       "  '2',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '2019',\n",
       "  '\"January\"',\n",
       "  '\"Tuesday\"',\n",
       "  '\"05:15 am\"',\n",
       "  '\"110\"',\n",
       "  '\"Divided Road\"',\n",
       "  '\"Straight road\"',\n",
       "  '\"Slope\"',\n",
       "  '\"Not Applicable\"',\n",
       "  '\"Sealed\"',\n",
       "  '\"Dry\"',\n",
       "  '\"Not Raining\"',\n",
       "  '\"Night\"',\n",
       "  '\"Hit Animal\"',\n",
       "  '\"02\"',\n",
       "  '\"Animal\"',\n",
       "  '\"1: PDO\"',\n",
       "  '\"No Control\"',\n",
       "  '\"\"',\n",
       "  '\"\"',\n",
       "  '1353279.99',\n",
       "  '1655645.15',\n",
       "  '\"13532801655645\"'],\n",
       " ['\"2019-6-8/07/2020\"',\n",
       "  '\"2 Metropolitan\"',\n",
       "  '\"TORRENSVILLE\"',\n",
       "  '\"5031\"',\n",
       "  '\"CITY OF WEST TORRENS\"',\n",
       "  '2',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2019',\n",
       "  '\"January\"',\n",
       "  '\"Tuesday\"',\n",
       "  '\"07:00 am\"',\n",
       "  '\"050\"',\n",
       "  '\"Divided Road\"',\n",
       "  '\"Straight road\"',\n",
       "  '\"Level\"',\n",
       "  '\"Not Applicable\"',\n",
       "  '\"Sealed\"',\n",
       "  '\"Dry\"',\n",
       "  '\"Not Raining\"',\n",
       "  '\"Daylight\"',\n",
       "  '\"Hit Fixed Object\"',\n",
       "  '\"01\"',\n",
       "  '\"Driver Rider\"',\n",
       "  '\"2: MI\"',\n",
       "  '\"No Control\"',\n",
       "  '\"\"',\n",
       "  '\"\"',\n",
       "  '1324652.75',\n",
       "  '1672027.64',\n",
       "  '\"13246531672028\"'],\n",
       " ['\"2019-7-8/07/2020\"',\n",
       "  '\"2 Metropolitan\"',\n",
       "  '\"BEDFORD PARK\"',\n",
       "  '\"5042\"',\n",
       "  '\"CC MITCHAM.                   \"',\n",
       "  '2',\n",
       "  '3',\n",
       "  '0',\n",
       "  '0',\n",
       "  '3',\n",
       "  '2019',\n",
       "  '\"January\"',\n",
       "  '\"Tuesday\"',\n",
       "  '\"09:40 am\"',\n",
       "  '\"050\"',\n",
       "  '\"Cross Road\"',\n",
       "  '\"Straight road\"',\n",
       "  '\"Level\"',\n",
       "  '\"Not Applicable\"',\n",
       "  '\"Sealed\"',\n",
       "  '\"Dry\"',\n",
       "  '\"Not Raining\"',\n",
       "  '\"Daylight\"',\n",
       "  '\"Right Angle\"',\n",
       "  '\"02\"',\n",
       "  '\"Driver Rider\"',\n",
       "  '\"2: MI\"',\n",
       "  '\"Traffic Signals\"',\n",
       "  '\"\"',\n",
       "  '\"\"',\n",
       "  '1325156.2',\n",
       "  '1660414.38',\n",
       "  '\"13251561660414\"'],\n",
       " ['\"2019-8-8/07/2020\"',\n",
       "  '\"3 Country\"',\n",
       "  '\"WYE\"',\n",
       "  '\"5291\"',\n",
       "  '\"DISTRICT COUNCIL OF GRANT\"',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2019',\n",
       "  '\"January\"',\n",
       "  '\"Tuesday\"',\n",
       "  '\"12:15 pm\"',\n",
       "  '\"110\"',\n",
       "  '\"Not Divided\"',\n",
       "  '\"CURVED',\n",
       "  ' VIEW OPEN\"',\n",
       "  '\"Level\"',\n",
       "  '\"Not Applicable\"',\n",
       "  '\"Sealed\"',\n",
       "  '\"Dry\"',\n",
       "  '\"Not Raining\"',\n",
       "  '\"Daylight\"',\n",
       "  '\"Roll Over\"',\n",
       "  '\"01\"',\n",
       "  '\"Driver Rider\"',\n",
       "  '\"2: MI\"',\n",
       "  '\"No Control\"',\n",
       "  '\"\"',\n",
       "  '\"\"',\n",
       "  '1517347.8',\n",
       "  '1321979.33',\n",
       "  '\"15173481321979\"'],\n",
       " ['\"2019-9-8/07/2020\"',\n",
       "  '\"3 Country\"',\n",
       "  '\"MOUNT GAMBIER\"',\n",
       "  '\"5290\"',\n",
       "  '\"CC MT.GAMBIER.                \"',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2019',\n",
       "  '\"January\"',\n",
       "  '\"Tuesday\"',\n",
       "  '\"11:45 am\"',\n",
       "  '\"050\"',\n",
       "  '\"T-Junction\"',\n",
       "  '\"Straight road\"',\n",
       "  '\"Bottom of Hill\"',\n",
       "  '\"Not Applicable\"',\n",
       "  '\"Sealed\"',\n",
       "  '\"Dry\"',\n",
       "  '\"Not Raining\"',\n",
       "  '\"Daylight\"',\n",
       "  '\"Roll Over\"',\n",
       "  '\"01\"',\n",
       "  '\"Driver Rider\"',\n",
       "  '\"2: MI\"',\n",
       "  '\"Stop Sign\"',\n",
       "  '\"\"',\n",
       "  '\"\"',\n",
       "  '1510220.05',\n",
       "  '1340472.35',\n",
       "  '\"15102201340472\"'],\n",
       " ['\"2019-10-8/07/2020\"',\n",
       "  '\"3 Country\"',\n",
       "  '\"OVERLAND CORNER\"',\n",
       "  '\"5330\"',\n",
       "  '\"THE BERRI BARMERA COUNCIL\"',\n",
       "  '1',\n",
       "  '1',\n",
       "  '0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '2019',\n",
       "  '\"January\"',\n",
       "  '\"Tuesday\"',\n",
       "  '\"12:30 pm\"',\n",
       "  '\"080\"',\n",
       "  '\"Not Divided\"',\n",
       "  '\"CURVED',\n",
       "  ' VIEW OPEN\"',\n",
       "  '\"Slope\"',\n",
       "  '\"Not Applicable\"',\n",
       "  '\"Sealed\"',\n",
       "  '\"Dry\"',\n",
       "  '\"Not Raining\"',\n",
       "  '\"Daylight\"',\n",
       "  '\"Roll Over\"',\n",
       "  '\"01\"',\n",
       "  '\"Driver Rider\"',\n",
       "  '\"3: SI\"',\n",
       "  '\"No Control\"',\n",
       "  '\"\"',\n",
       "  '\"\"',\n",
       "  '1492599.9',\n",
       "  '1749395.71',\n",
       "  '\"14926001749396\"']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Crash RDD\n",
    "\n",
    "# Split each line separated by comma into a list \n",
    "crashrdd = crash_rdd.map(lambda line: line.split(','))\n",
    "# Remove the header\n",
    "header = crashrdd.first()\n",
    "crash_final = crashrdd.filter(lambda row: row != header)   #filter out header\n",
    "# Print how many final records\n",
    "print(f\"\\n Total count of Crashes : {crash_final.count()}\")\n",
    "crash_final.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Partitioning in RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Check the number of partitions in the above RDDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF PARTITIONS IN UNITS: 5\n",
      "Partitioner:None\n",
      "Partition 0: 35861 records\n",
      "Partition 1: 28163 records\n",
      "Partition 2: 33084 records\n",
      "Partition 3: 27713 records\n",
      "Partition 4: 29033 records\n",
      "NUMBER OF PARTITIONS IN CRASH: 5\n",
      "Partition 0: 12965 records\n",
      "Partition 1: 16776 records\n",
      "Partition 2: 13238 records\n",
      "Partition 3: 13600 records\n",
      "Partition 4: 15432 records\n"
     ]
    }
   ],
   "source": [
    "from pyspark.rdd import RDD\n",
    "# Calculating and printing number of partitions and partitioner of units RDD\n",
    "numPartitions_units = units_final.getNumPartitions()\n",
    "print(f\"NUMBER OF PARTITIONS IN UNITS: {numPartitions_units}\")\n",
    "print(\"Partitioner:{}\".format(units_final.partitioner))\n",
    "\n",
    "# Building a function to get number of records per transition\n",
    "def print_partitions(data):\n",
    "    partitions = data.glom().collect()\n",
    "    for index, partition in enumerate(partitions):\n",
    "        # show partition if it is not empty\n",
    "        if len(partition) > 0:\n",
    "            print(f\"Partition {index}: {len(partition)} records\")\n",
    "\n",
    "# Printing records per partition using the above function\n",
    "print_partitions(units_final)            \n",
    "\n",
    "# Printing number of partitions and records per transition for each partition for crash rdd\n",
    "numPartitions_crash = crash_final.getNumPartitions()\n",
    "print(f\"NUMBER OF PARTITIONS IN CRASH: {numPartitions_crash}\")\n",
    "print_partitions(crash_final) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observing the records in each of the partitions of Units and Crash RDD, we can say that records are distibuted almost equally in all the 5 partitions in both Units and Crash RDD. Thus, we can conclude that Random equal partitioning is used by default in the partitioning both Units and Crash RDDs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### In the “Units” csv dataset, there is a column called Lic State which shows the state where the vehicle is registered. Assume we want to keep all the data related to SA in one partition and the rest of the data in another partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### a. Create a Key Value Pair RDD with Lic State as the key and rest of the other columns as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new RDD with Lic State as key and other columns as values\n",
    "units_pairs = units_final.map(lambda x: (x[9], [x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7],\n",
    "                                         x[8],x[10],x[11],x[12],x[13],x[14],x[15],x[16],x[17]]))\n",
    "\n",
    "# units_pairs.collect() # Displaying the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Implement this partitioning in RDD using appropriate partitioning functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating \"SA\"in one partition and other states in other partition using Hash function\n",
    "# Building a hash function that accepts key of the RDD as input\n",
    "def hash_function(key):\n",
    "    if key == '\"SA\"':\n",
    "        total = 2 # if key is \"SA\" then a total 2 is returned, which is kept in partition 2%2=0\n",
    "    else:\n",
    "        total = 3 # else a total 3 is returned, which is kept in partition 3%2=1\n",
    "    return total\n",
    "\n",
    "# generating 2 partitions by hash_function \n",
    "hash_units_rdd = units_pairs.partitionBy(2,hash_function) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Print the number of records in each partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: 109684 records\n",
      "Partition 1: 44170 records\n"
     ]
    }
   ],
   "source": [
    "# Printing record in each partition of the RDD generated in previous part\n",
    "partitions = hash_units_rdd.glom().collect()\n",
    "for index, partition in enumerate(partitions):\n",
    "    print(f\"Partition {index}: {len(partition)} records\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observing, we can see that there are 109684 records of \"SA\" state and 44170 records containing other states. This maybe because of more data of \"SA\" in the datasets compared to other states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Query/Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calculate average age of male and female drivers separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Unknown', 32.55813953488372),\n",
       " ('Male', 40.975960299920004),\n",
       " ('Female', 40.38729268862415)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting Sex and Age column from units RDD and removing extra double quotes from the values\n",
    "units_sex_pairs = units_final.map(lambda x: (x[7].replace('\"',''), x[8].replace('\"','')))\n",
    "\n",
    "# Removing null values of the 2 columns and filtering out \"XXX\" values in Age column\n",
    "units_sex_pairs = units_sex_pairs.filter(lambda x: x[0] != '' and x[1]!= '')\n",
    "units_sex_pairs = units_sex_pairs.filter(lambda x: x[1] != 'XXX')\n",
    "units_sex_pairs = units_sex_pairs.map(lambda x: (x[0], int(x[1]))) # convert age to int\n",
    "\n",
    "# Grouping by key ,i.e.,Sex, and calculating average age of each sex group\n",
    "units_male_avg = units_sex_pairs.groupByKey().map(lambda x: (x[0], sum(x[1])/len(x[1])))\n",
    "units_male_avg.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result displayed, we can observe that average age of Male is around 40 years and that of Female is around 41 years. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calculate the oldest and the newest vehicle year involved in the accident and find their Registration State, Year and Unit type of the vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('VIC', 1900, 'Motor Cycle')\n",
      "('SA', 2019, 'Station Wagon')\n"
     ]
    }
   ],
   "source": [
    "# Selecting the required columns and removing quotes, null values and 'XXXX' values\n",
    "units_veh_pairs = units_final.map(lambda x: (x[3].replace('\"',''), x[5].replace('\"','')\n",
    "                                            , x[4].replace('\"','')))\n",
    "units_veh_pairs = units_veh_pairs.filter(lambda x: x[1]!= '')\n",
    "units_veh_pairs = units_veh_pairs.filter(lambda x: x[1] != 'XXXX')\n",
    "units_veh_pairs = units_veh_pairs.map(lambda x: (x[0], int(x[1]), x[2])) # making year column an integer\n",
    "\n",
    "#units_veh_pairs.collect()\n",
    "# Getting oldest vehicle with minimum year and printing the result\n",
    "oldest_veh = units_veh_pairs.min(key=lambda x: x[1]) \n",
    "print(oldest_veh)\n",
    "\n",
    "# Getting newest vehicle with maximum year and printing the result\n",
    "newest_veh = units_veh_pairs.max(key=lambda x: x[1])\n",
    "print(newest_veh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the oldest year of the vehile is 1900 while the newest year involved is 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('VIC', 1900, 'Motor Cycle'),\n",
       " ('SA', 1900, 'Motor Cycle'),\n",
       " ('SA', 1900, 'Motor Cycle'),\n",
       " ('SA', 1900, 'Motor Cycle'),\n",
       " ('SA', 1900, 'Motor Cycle'),\n",
       " ('SA', 1900, 'Motor Cycle'),\n",
       " ('SA', 1900, 'Motor Cycle'),\n",
       " ('SA', 1900, 'RIGID TRUCK LGE GE 4.5T'),\n",
       " ('SA', 1900, 'Motor Cycle'),\n",
       " ('SA', 1900, 'Motor Cycle'),\n",
       " ('SA', 1900, 'Motor Cycle'),\n",
       " ('SA', 1900, 'Motor Cycle'),\n",
       " ('SA', 1900, 'Motor Cycle'),\n",
       " ('SA', 1900, 'Motor Cycle')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the data of all vehicles in 1900\n",
    "old = units_veh_pairs.filter(lambda x: x[1]== 1900)\n",
    "old.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that during 1900 the most common vehicle type is a Motor Cycle, and the most common licence state is SA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting the data of all vehicles in 2019\n",
    "new = units_veh_pairs.filter(lambda x: x[1]== 2019)\n",
    "# new.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2019, we can see a wide variety in the vehicle types, but the most common licence state is still SA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Preparation and Loading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load all units and crash data into two separate dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating units dataframe using * \n",
    "units_df = spark.read.csv(\"*Units.csv\",header=True)\n",
    "crash_df = spark.read.csv(\"*Crash.csv\",header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Display the schema of the final two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- REPORT_ID: string (nullable = true)\n",
      " |-- Unit No: string (nullable = true)\n",
      " |-- No Of Cas: string (nullable = true)\n",
      " |-- Veh Reg State: string (nullable = true)\n",
      " |-- Unit Type: string (nullable = true)\n",
      " |-- Veh Year: string (nullable = true)\n",
      " |-- Direction Of Travel: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Lic State: string (nullable = true)\n",
      " |-- Licence Class: string (nullable = true)\n",
      " |-- Licence Type: string (nullable = true)\n",
      " |-- Towing: string (nullable = true)\n",
      " |-- Unit Movement: string (nullable = true)\n",
      " |-- Number Occupants: string (nullable = true)\n",
      " |-- Postcode: string (nullable = true)\n",
      " |-- Rollover: string (nullable = true)\n",
      " |-- Fire: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- REPORT_ID: string (nullable = true)\n",
      " |-- Stats Area: string (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Postcode: string (nullable = true)\n",
      " |-- LGA Name: string (nullable = true)\n",
      " |-- Total Units: string (nullable = true)\n",
      " |-- Total Cas: string (nullable = true)\n",
      " |-- Total Fats: string (nullable = true)\n",
      " |-- Total SI: string (nullable = true)\n",
      " |-- Total MI: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Area Speed: string (nullable = true)\n",
      " |-- Position Type: string (nullable = true)\n",
      " |-- Horizontal Align: string (nullable = true)\n",
      " |-- Vertical Align: string (nullable = true)\n",
      " |-- Other Feat: string (nullable = true)\n",
      " |-- Road Surface: string (nullable = true)\n",
      " |-- Moisture Cond: string (nullable = true)\n",
      " |-- Weather Cond: string (nullable = true)\n",
      " |-- DayNight: string (nullable = true)\n",
      " |-- Crash Type: string (nullable = true)\n",
      " |-- Unit Resp: string (nullable = true)\n",
      " |-- Entity Code: string (nullable = true)\n",
      " |-- CSEF Severity: string (nullable = true)\n",
      " |-- Traffic Ctrls: string (nullable = true)\n",
      " |-- DUI Involved: string (nullable = true)\n",
      " |-- Drugs Involved: string (nullable = true)\n",
      " |-- ACCLOC_X: string (nullable = true)\n",
      " |-- ACCLOC_Y: string (nullable = true)\n",
      " |-- UNIQUE_LOC: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying schema of both dataframes \n",
    "units_df.printSchema()\n",
    "crash_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Query/Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Find all the crash events in Adelaide where the total number of casualties in the event is more than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "|           REPORT_ID|Stats Area|  Suburb|Postcode|        LGA Name|Total Units|Total Cas|Total Fats|Total SI|Total MI|Year|   Month|     Day|    Time|Area Speed|Position Type|Horizontal Align|Vertical Align|          Other Feat|Road Surface|Moisture Cond|Weather Cond|DayNight|    Crash Type|Unit Resp| Entity Code|CSEF Severity|  Traffic Ctrls|DUI Involved|Drugs Involved|  ACCLOC_X|  ACCLOC_Y|    UNIQUE_LOC|\n",
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "| 2018-601-17/01/2020|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          8|        4|         0|       2|       2|2018| January|  Sunday|09:12 pm|       050|  Not Divided|   Straight road|         Level|      Not Applicable|      Sealed|          Dry| Not Raining|   Night|Hit Pedestrian|       01|Driver Rider|        3: SI|     No Control|        null|          null|1329806.36|1670224.76|13298061670225|\n",
      "|2017-1613-15/08/2019|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          2|        4|         0|       0|       4|2017|February|Saturday|04:00 pm|       050|   Cross Road|   Straight road|         Level|      Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Right Turn|       01|Driver Rider|        2: MI|Traffic Signals|        null|          null|1327951.24|1669556.92|13279511669557|\n",
      "|2017-12182-15/08/...|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          6|        5|         0|       1|       4|2017|December|Saturday|04:08 pm|       050|   Cross Road|   Straight road|         Level|      Not Applicable|      Sealed|          Wet| Not Raining|Daylight|Hit Pedestrian|       01|Driver Rider|        3: SI|Traffic Signals|        null|          null| 1329016.2|1670995.07|13290161670995|\n",
      "|2019-10404-8/07/2020|    1 City|ADELAIDE|    5000|CITY OF ADELAIDE|          4|        6|         0|       0|       6|2019| October|  Monday|08:20 am|       060| Divided Road|   Straight road|         Level|Driveway or Entrance|      Sealed|          Dry| Not Raining|Daylight|    Right Turn|       01|Driver Rider|        2: MI|     No Control|        null|          null|1327088.72|1670880.07|13270891670880|\n",
      "+--------------------+----------+--------+--------+----------------+-----------+---------+----------+--------+--------+----+--------+--------+--------+----------+-------------+----------------+--------------+--------------------+------------+-------------+------------+--------+--------------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing functions from sql and Integer Type\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Converting Total Cas column to an integer type\n",
    "crash_df = crash_df.withColumn('Total Cas',F.col('Total Cas').cast(IntegerType()))\n",
    "\n",
    "# Filtering for suburb Adelaide and for Total Cas greater than 3\n",
    "crash_cas_adel = crash_df.filter(crash_df.Suburb == 'ADELAIDE') \n",
    "crash_cas_adel = crash_cas_adel.filter(crash_df[\"Total Cas\"] > 3)\n",
    "crash_cas_adel.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Display 10 crash events with highest casualties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+---------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "|           REPORT_ID|    Stats Area|         Suburb|Postcode|            LGA Name|Total Units|Total Cas|Total Fats|Total SI|Total MI|Year|   Month|      Day|    Time|Area Speed|Position Type|    Horizontal Align|Vertical Align|    Other Feat|Road Surface|Moisture Cond|Weather Cond|DayNight| Crash Type|Unit Resp| Entity Code|CSEF Severity|  Traffic Ctrls|DUI Involved|Drugs Involved|  ACCLOC_X|  ACCLOC_Y|    UNIQUE_LOC|\n",
      "+--------------------+--------------+---------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "| 2017-288-15/08/2019|2 Metropolitan|     PARA HILLS|    5096|   CITY OF SALISBURY|          2|       11|         0|       1|      10|2017| January|Wednesday|01:13 pm|       060|   T-Junction|       Straight road| Crest of Hill|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|Right Angle|       01|Driver Rider|        3: SI|      Stop Sign|        null|          null| 1334428.9|1683032.96|13344291683033|\n",
      "|2016-3035-15/08/2019|2 Metropolitan|        HACKHAM|    5163| CITY OF ONKAPARINGA|          3|        9|         3|       5|       1|2016| January| Saturday|11:50 am|       080|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight| Right Turn|       01|Driver Rider|     4: Fatal|     No Control|        null|          null|1320361.49|1645195.63|13203611645196|\n",
      "|2016-6630-15/08/2019|2 Metropolitan|  KANGAROO FLAT|    5118|LIGHT REGIONAL CO...|          3|        9|         0|       2|       7|2016|   April|Wednesday|09:00 pm|       100|  Not Divided|CURVED, VIEW OBSC...|         Level|Not Applicable|      Sealed|          Dry| Not Raining|   Night|    Head On|       01|Driver Rider|        3: SI|     No Control|        null|          null|1339316.32|1710314.92|13393161710315|\n",
      "|2019-11734-8/07/2020|2 Metropolitan|          STURT|    5047|CC MARION.       ...|          2|        9|         0|       1|       8|2019|November|   Sunday|07:25 pm|       060|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight| Right Turn|       02|Driver Rider|        3: SI|Traffic Signals|        null|          null|1324428.84|1659884.95|13244291659885|\n",
      "|2016-14407-15/08/...|     3 Country|      STOCKWELL|    5355|THE BAROSSA COUNCIL.|          2|        8|         1|       6|       1|2016| October|   Sunday|03:46 pm|       100|  Not Divided|       Straight road| Crest of Hill|Not Applicable|    Unsealed|          Dry| Not Raining|Daylight|    Head On|       01|Driver Rider|     4: Fatal|     No Control|        null|          null|1373964.45|1723462.57|13739641723463|\n",
      "|2016-7073-15/08/2019|     3 Country|       MERRITON|    5523|PT.PIRIE CITY & D...|          2|        8|         4|       3|       1|2016|   April|   Sunday|12:35 pm|       110|  Not Divided|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Head On|       01|Driver Rider|     4: Fatal|     No Control|        null|          null|1293759.89|1840109.96|12937601840110|\n",
      "|2015-2823-21/08/2019|     3 Country|         HAWKER|    5434|THE FLINDERS RANG...|          1|        8|         0|       0|       8|2015|   March|   Monday|06:00 pm|       110|  Not Divided|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|  Roll Over|       01|Driver Rider|        2: MI|     No Control|        null|          null|1315077.61|2022309.34|13150782022309|\n",
      "|2015-12591-21/08/...|     3 Country|        MALLALA|    5502|DC MALLALA.      ...|          2|        7|         0|       2|       5|2015| October|   Sunday|02:30 pm|       100|   Cross Road|       Straight road|         Level|Not Applicable|    Unsealed|          Dry| Not Raining|Daylight|Right Angle|       01|Driver Rider|        3: SI|  Give Way Sign|        null|          null|1325122.01|1724860.95|13251221724861|\n",
      "|2015-13713-21/08/...|2 Metropolitan|ELIZABETH GROVE|    5112|   CITY OF PLAYFORD.|          2|        7|         0|       0|       7|2015|November|   Friday|03:42 pm|       080|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|   Rear End|       01|Driver Rider|        2: MI|     No Control|        null|          null|1336118.68|1691385.65|13361191691386|\n",
      "|2015-6965-21/08/2019|     3 Country|       BEAUFORT|    5550|YORKE PENINSULA C...|          3|        7|         3|       4|       0|2015|    June|   Monday|11:13 am|       100|   T-Junction|       Straight road|         Level|Not Applicable|      Sealed|          Dry| Not Raining|Daylight|    Head On|       09|       Other|     4: Fatal|     No Control|        null|          null|1287930.19|1761652.36|12879301761652|\n",
      "+--------------------+--------------+---------------+--------+--------------------+-----------+---------+----------+--------+--------+----+--------+---------+--------+----------+-------------+--------------------+--------------+--------------+------------+-------------+------------+--------+-----------+---------+------------+-------------+---------------+------------+--------------+----------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sorting the Total Cas in descending order and printing first 10 highest casualities\n",
    "high_cas = crash_df.sort(crash_df[\"Total Cas\"].desc())\n",
    "high_cas.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Find the total number of fatalities for each crash type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+\n",
      "|          Crash Type|Total number of Fatalities|\n",
      "+--------------------+--------------------------+\n",
      "|    Hit Fixed Object|                       152|\n",
      "|             Head On|                        86|\n",
      "|      Hit Pedestrian|                        70|\n",
      "|           Roll Over|                        57|\n",
      "|         Right Angle|                        45|\n",
      "|          Side Swipe|                        20|\n",
      "|          Right Turn|                        18|\n",
      "|            Rear End|                        16|\n",
      "|  Hit Parked Vehicle|                         9|\n",
      "|          Hit Animal|                         4|\n",
      "|               Other|                         2|\n",
      "|  Hit Object on Road|                         2|\n",
      "|Left Road - Out o...|                         1|\n",
      "+--------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting Total Fats to integer type\n",
    "crash_df = crash_df.withColumn('Total Fats',F.col('Total Fats').cast(IntegerType()))\n",
    "\n",
    "# Grouping by crash type and summing the fatalities for each crash type\n",
    "typewise_fats = crash_df.groupby(\"Crash Type\").agg(F.sum(\"Total Fats\").alias('Total number of Fatalities'))\n",
    "# Sorting in descending order to get better insight\n",
    "typewise_fats = typewise_fats.sort(typewise_fats[\"Total number of Fatalities\"].desc())\n",
    "typewise_fats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that most casualities are from Hit fixed object, head on, hit pedestrian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Find the total number of casualties for each suburb when the vehicle was driven by an unlicensed driver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------------+\n",
      "|         Suburb|Total number of Casualities|\n",
      "+---------------+---------------------------+\n",
      "|       ADELAIDE|                         19|\n",
      "|      SALISBURY|                         18|\n",
      "|      DRY CREEK|                         18|\n",
      "| SALISBURY EAST|                         16|\n",
      "|       PROSPECT|                         14|\n",
      "| NORTH ADELAIDE|                         13|\n",
      "|        ENFIELD|                         12|\n",
      "|   ANDREWS FARM|                         12|\n",
      "|SALISBURY DOWNS|                         11|\n",
      "|   BEDFORD PARK|                         11|\n",
      "|SALISBURY SOUTH|                         11|\n",
      "|     INGLE FARM|                         11|\n",
      "|     MUNNO PARA|                         10|\n",
      "|         BURTON|                         10|\n",
      "|SALISBURY PLAIN|                         10|\n",
      "|   MOUNT BARKER|                         10|\n",
      "| ELIZABETH PARK|                         10|\n",
      "|  MORPHETT VALE|                         10|\n",
      "|   MAWSON LAKES|                         10|\n",
      "| ELIZABETH EAST|                          9|\n",
      "+---------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining units_df and crash_df\n",
    "units_crash_joined = units_df.join(crash_df, units_df.REPORT_ID==crash_df.REPORT_ID,how='inner')\n",
    "\n",
    "# Filtering for unlicenced licence type and grouping by suburb and adding casualities in each group\n",
    "sub_cas = units_crash_joined.filter(units_crash_joined[\"Licence Type\"] == \"Unlicenced\").select(\"Suburb\",\"Total Cas\").groupby(\"Suburb\").agg(F.sum(\"Total Cas\").alias('Total number of Casualities'))\n",
    "sub_cas.sort(sub_cas[\"Total number of Casualities\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of casualities with an unlicenced driver is highest in Adelaide with total 19 casualities followed by Salisbury and Dry Creek."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Severity Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Find the total number of crash events for each severity level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------------+\n",
      "|CSEF Severity|Total number of Crash events|\n",
      "+-------------+----------------------------+\n",
      "|       1: PDO|                       46696|\n",
      "|        2: MI|                       21881|\n",
      "|        3: SI|                        2978|\n",
      "|     4: Fatal|                         451|\n",
      "+-------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping the dataframe by security level and counting records in each group\n",
    "crash_severity = crash_df.groupby(\"CSEF Severity\").agg(F.count(\"*\").alias('Total number of Crash events'))\n",
    "crash_severity.sort(crash_severity[\"Total number of Crash events\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, we can notice that 1:PDO crash severity, i.e., property damage only is the most common severity level. This is followed by 2:MI, i.e., minor injury, followed by 3:SI, i.e., serious injury and 4:FATAL. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compute the total number of crash events for each severity level and the percentage for the four different scenarios.\n",
    "###### a. When the driver is tested positive on drugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+-------------------+\n",
      "|CSEF Severity|Count on drugs|Percentage on drugs|\n",
      "+-------------+--------------+-------------------+\n",
      "|        2: MI|           749|            59.73%\n",
      "|\n",
      "|        3: SI|           247|            19.70%\n",
      "|\n",
      "|       1: PDO|           176|            14.04%\n",
      "|\n",
      "|     4: Fatal|            82|             6.54%\n",
      "|\n",
      "+-------------+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing functions \n",
    "from pyspark.sql.functions import rank,sum,col\n",
    "# Filtering, grouping and summing based on the conditions\n",
    "drug_positive = crash_df.filter(crash_df[\"Drugs Involved\"] == \"Y\").groupby(\"CSEF Severity\").agg(F.count(\"*\").alias('Count on drugs'))\n",
    "# Creating a new Percentage column and formatting it \n",
    "Total = drug_positive.agg(F.sum('Count on drugs').alias('Total')).collect()[0][0] \n",
    "drug_positive = drug_positive.withColumn('Percentage on drugs', F.format_string(\"%2.2f%%\\n\", col('Count on drugs')/Total * 100))\n",
    "drug_positive.sort(drug_positive[\"Count on drugs\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### b. When the driver is tested positive for blood alcohol concentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+---------------------+\n",
      "|CSEF Severity|Count on alcohol|Percentage on alcohol|\n",
      "+-------------+----------------+---------------------+\n",
      "|       1: PDO|            1173|              52.18%\n",
      "|\n",
      "|        2: MI|             737|              32.78%\n",
      "|\n",
      "|        3: SI|             259|              11.52%\n",
      "|\n",
      "|     4: Fatal|              79|               3.51%\n",
      "|\n",
      "+-------------+----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering, grouping and summing based on the conditions\n",
    "alco_positive = crash_df.filter(crash_df[\"DUI Involved\"] == \"Y\").groupby(\"CSEF Severity\").agg(F.count(\"*\").alias('Count on alcohol'))\n",
    "# Creating a new Percentage column and formatting it \n",
    "Total = alco_positive.agg(F.sum('Count on alcohol').alias('Total')).collect()[0][0] \n",
    "alco_positive = alco_positive.withColumn('Percentage on alcohol', F.format_string(\"%2.2f%%\\n\", col('Count on alcohol')/Total * 100))\n",
    "alco_positive.sort(alco_positive[\"Count on alcohol\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### c. When the driver is tested positive for both drugs and blood alcohol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+------------------+\n",
      "|CSEF Severity|Count on both|Percentage on both|\n",
      "+-------------+-------------+------------------+\n",
      "|        2: MI|           89|           50.86%\n",
      "|\n",
      "|        3: SI|           35|           20.00%\n",
      "|\n",
      "|     4: Fatal|           27|           15.43%\n",
      "|\n",
      "|       1: PDO|           24|           13.71%\n",
      "|\n",
      "+-------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering, grouping and summing based on the conditions\n",
    "drug_alco_pos = crash_df.filter((crash_df[\"DUI Involved\"] == \"Y\") & (crash_df[\"Drugs Involved\"] == \"Y\")).groupby(\"CSEF Severity\").agg(F.count(\"*\").alias('Count on both'))\n",
    "# Creating a new Percentage column and formatting it \n",
    "Total = drug_alco_pos.agg(F.sum('Count on both').alias('Total')).collect()[0][0] \n",
    "drug_alco_pos = drug_alco_pos.withColumn('Percentage on both', F.format_string(\"%2.2f%%\\n\", col('Count on both')/Total * 100))\n",
    "drug_alco_pos.sort(drug_alco_pos[\"Count on both\"].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### d. When the driver is tested negative for both (no alcohol and no drugs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|DUI Involved|\n",
      "+------------+\n",
      "|        null|\n",
      "|           Y|\n",
      "+------------+\n",
      "\n",
      "+--------------+\n",
      "|Drugs Involved|\n",
      "+--------------+\n",
      "|          null|\n",
      "|             Y|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crash_df.select(\"DUI Involved\").distinct().show()\n",
    "crash_df.select(\"Drugs Involved\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+----------------------+\n",
      "|CSEF Severity|Count on negative|Percentage on negative|\n",
      "+-------------+-----------------+----------------------+\n",
      "|       1: PDO|            45371|               66.06%\n",
      "|\n",
      "|        2: MI|            20484|               29.83%\n",
      "|\n",
      "|        3: SI|             2507|                3.65%\n",
      "|\n",
      "|     4: Fatal|              317|                0.46%\n",
      "|\n",
      "+-------------+-----------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering, grouping and summing based on the conditions\n",
    "drug_alco_neg = crash_df.select(\"CSEF Severity\", \"DUI Involved\",\"Drugs Involved\").filter((F.isnull(\"DUI Involved\"))& (F.isnull(\"Drugs Involved\"))).groupby(\"CSEF Severity\").agg(F.count(\"*\").alias('Count on negative'))\n",
    "# Creating a new Percentage column and formatting it \n",
    "Total = drug_alco_neg.agg(F.sum('Count on negative').alias('Total')).collect()[0][0] \n",
    "drug_alco_neg = drug_alco_neg.withColumn('Percentage on negative', F.format_string(\"%2.2f%%\\n\", col('Count on negative')/Total * 100))\n",
    "drug_alco_neg.sort(drug_alco_neg[\"Count on negative\"].desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Comparing the results in these 4 scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we observe that almost 60% of people tested positive for drugs were involved in minor injury, 52% of people tested positive for alcohol were involved in property damage, 51% of people that tested positive for both were involved in minor injury, and 66% of people tested negative for both caused property damage. Among the 4 scenarios, highest fatality rate was of 15% among people tested positive for both drug and alcohol followed by 6.54% of fatality rate of people taking drugs. Highest rate of serious injury was 20% found among people that tested positive for both, and was followed by 19.7% of serious injury rate of people taking drugs. From the calculated percentages, we can say that as people involved in both drugs and alcohol and people involvedin drugs have high risk of a fatal or serious crash. People involved with alcohol also have a risk of causing crash. While, people not involved in both drugs and alcohol have low chances of getting into a fatal crash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 RDDs vs DataFrame vs Spark SQL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Find the Date1 and Time of Crash, Number of Casualties in each unit and the Gender, Age, License Type of the unit driver for the suburb \"Adelaide\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (A) Using RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Getting the required columns from the units RDD and formatting them\n",
    "adel_units_rdd = units_final.map(lambda field: (field[0],field[7],field[8],field[11]))\n",
    "adel_units_rdd = adel_units_rdd.map(lambda x: (x[0].replace('\"',''), (x[1].replace('\"','') \\\n",
    "                                               , x[2].replace('\"',''), x[3].replace('\"',''))))\n",
    "# Getting the required columns from the crash RDD and formatting them\n",
    "adel_crash_rdd = crash_final.map(lambda field: (field[0],field[2],field[6]\\\n",
    "                                              ,field[10],field[11],field[12],field[13]))\n",
    "adel_crash_rdd = adel_crash_rdd.map(lambda x: (x[0].replace('\"',''),(x[1].replace('\"','')\\\n",
    "                                               ,x[2].replace('\"',''),x[3].replace('\"','')\\\n",
    "                                               ,x[4].replace('\"',''),x[5].replace('\"','')\\\n",
    "                                               ,x[6].replace('\"',''))))\n",
    "# Filtering the suburb\n",
    "adel_crash_rdd = adel_crash_rdd.filter(lambda x: x[1][0]=='ADELAIDE')\n",
    "# joining the two RDDs\n",
    "join_rdd = adel_units_rdd.join(adel_crash_rdd)\n",
    "\n",
    "# Formatting the columns and printing them\n",
    "# join_rdd.map(lambda x: [x[1][0][0],x[1][0][1],x[1][0][2], x[1][1][1],\\\n",
    "#                         x[1][1][2]+\"-\"+x[1][1][3]+\"-\"+x[1][1][4],\\\n",
    "#                         x[1][1][5]]).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (B) Using Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+-------+----+------------+\n",
      "|                Date|    Time|Total Cas|    Sex| Age|Licence Type|\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "|2016-November-Wed...|01:45 pm|        1|   Male| 056|        Full|\n",
      "|2016-November-Wed...|01:45 pm|        1|   Male| 072|        null|\n",
      "|2016-November-Tue...|03:40 pm|        1|   Male| 056|        null|\n",
      "|2016-November-Tue...|03:40 pm|        1| Female| 027|        null|\n",
      "|2016-November-Tue...|05:00 pm|        0| Female| 032|        Full|\n",
      "|2016-November-Tue...|05:00 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Tue...|05:40 pm|        0|   Male| 022|     Unknown|\n",
      "|2016-November-Tue...|05:40 pm|        0|   Male| 020|     Unknown|\n",
      "|2016-November-Monday|11:26 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Monday|11:26 pm|        0|   Male| 042|        Full|\n",
      "|2016-November-Monday|11:26 pm|        0|   null|null|        null|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 026|     Unknown|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 038|        Full|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 036|        Full|\n",
      "|2016-November-Tue...|05:05 pm|        0|   Male| 025|     Unknown|\n",
      "|2016-November-Tue...|05:05 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Wed...|03:30 pm|        1| Female| 065|        Full|\n",
      "|2016-November-Wed...|03:30 pm|        1|Unknown| XXX|     Unknown|\n",
      "|2016-November-Wed...|02:20 pm|        0|   Male| 063|        Full|\n",
      "|2016-November-Wed...|02:20 pm|        0|   null|null|        null|\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 11.5 ms, sys: 4.8 ms, total: 16.3 ms\n",
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import concat, col, lit # Imprting required functions\n",
    "\n",
    "# Joining units_df and crash_df, and filter the suburb\n",
    "join_df = units_df.join(crash_df, units_df.REPORT_ID==crash_df.REPORT_ID,how='inner')\n",
    "join_df = join_df.filter(crash_df[\"Suburb\"] == \"ADELAIDE\")\n",
    "# Selecting and formatting the columns\n",
    "join_df.select(F.concat(col(\"Year\"), lit(\"-\"), col(\"Month\"),lit(\"-\"), col(\"Day\")).alias(\"Date\")\\\n",
    "              , \"Time\", \"Total Cas\", \"Sex\", \"Age\", \"Licence Type\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (C) Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+---------+-------+----+------------+\n",
      "|                Date|    Time|Total Cas|    Sex| Age|Licence Type|\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "|2016-November-Wed...|01:45 pm|        1|   Male| 056|        Full|\n",
      "|2016-November-Wed...|01:45 pm|        1|   Male| 072|        null|\n",
      "|2016-November-Tue...|03:40 pm|        1|   Male| 056|        null|\n",
      "|2016-November-Tue...|03:40 pm|        1| Female| 027|        null|\n",
      "|2016-November-Tue...|05:00 pm|        0| Female| 032|        Full|\n",
      "|2016-November-Tue...|05:00 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Tue...|05:40 pm|        0|   Male| 022|     Unknown|\n",
      "|2016-November-Tue...|05:40 pm|        0|   Male| 020|     Unknown|\n",
      "|2016-November-Monday|11:26 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Monday|11:26 pm|        0|   Male| 042|        Full|\n",
      "|2016-November-Monday|11:26 pm|        0|   null|null|        null|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 026|     Unknown|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 038|        Full|\n",
      "|2016-November-Monday|11:30 pm|        0|   Male| 036|        Full|\n",
      "|2016-November-Tue...|05:05 pm|        0|   Male| 025|     Unknown|\n",
      "|2016-November-Tue...|05:05 pm|        0|Unknown| XXX|     Unknown|\n",
      "|2016-November-Wed...|03:30 pm|        1| Female| 065|        Full|\n",
      "|2016-November-Wed...|03:30 pm|        1|Unknown| XXX|     Unknown|\n",
      "|2016-November-Wed...|02:20 pm|        0|   Male| 063|        Full|\n",
      "|2016-November-Wed...|02:20 pm|        0|   null|null|        null|\n",
      "+--------------------+--------+---------+-------+----+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 0 ns, sys: 8.61 ms, total: 8.61 ms\n",
      "Wall time: 3.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "units_df.createOrReplaceTempView(\"units_sql\") # Registering original df as a temp view\n",
    "crash_df.createOrReplaceTempView(\"crash_sql\")\n",
    "# Query to filter the suburb, format the columns\n",
    "filter_sql = spark.sql('''\n",
    "  SELECT crash_sql.Year||'-'||crash_sql.Month||'-'||crash_sql.Day as Date,\n",
    "  crash_sql.Time, crash_sql.`Total Cas`, units_sql.Sex,units_sql.Age, units_sql.`Licence Type`\n",
    "  FROM units_sql join crash_sql on units_sql.REPORT_ID == crash_sql.REPORT_ID\n",
    "  WHERE Suburb = \"ADELAIDE\" \n",
    "''')\n",
    "filter_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of time command, we can notice that the least wall time is observed when using dataframe, followed by SQL query, while RDD has the highest wall time. Similarly, least total time is observed in Sql query, followed by dataframe, and RDD has highest total time. From these observations, we can say that RDD are less efficient compared to SQL and Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Find the total number of casualties for each suburb when the vehicle was driven by an unlicensed driver. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (A) Using RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Getting the required columns from the units RDD and formatting them\n",
    "sub_units_rdd = units_final.map(lambda field: (field[0].replace('\"',''),(field[11].replace('\"',''))))\n",
    "# Getting the required columns from the crash RDD and formatting them\n",
    "sub_crash_rdd = crash_final.map(lambda field: (field[0].replace('\"',''),\\\n",
    "                                              (field[2].replace('\"',''),field[6].replace('\"',''))))\n",
    "\n",
    "# Joining the 2 RDDs \n",
    "join_rdd = sub_units_rdd.join(sub_crash_rdd)\n",
    "\n",
    "# Filtering the RDDs and formatting the columns\n",
    "filter_rdd = join_rdd.filter(lambda x: x[1][0]=='Unlicenced')\n",
    "filter_rdd = filter_rdd.map(lambda x: (x[1][1][0], (x[1][1][1])))\n",
    "filter_rdd = filter_rdd.filter(lambda x: x[0] != '' and x[1]!= '')\n",
    "filter_rdd = filter_rdd.map(lambda x: (x[0], int(x[1])))\n",
    "# Using groupByKey to get total values of each suburb\n",
    "result_rdd = filter_rdd.groupByKey().map(lambda x: (x[0], sum(x[1])))\n",
    "# result_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (B) Using Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------+\n",
      "|         Suburb|Number of casualities|\n",
      "+---------------+---------------------+\n",
      "|       ADELAIDE|                   19|\n",
      "|      SALISBURY|                   18|\n",
      "|      DRY CREEK|                   18|\n",
      "| SALISBURY EAST|                   16|\n",
      "|       PROSPECT|                   14|\n",
      "| NORTH ADELAIDE|                   13|\n",
      "|        ENFIELD|                   12|\n",
      "|   ANDREWS FARM|                   12|\n",
      "|SALISBURY DOWNS|                   11|\n",
      "|     INGLE FARM|                   11|\n",
      "|SALISBURY SOUTH|                   11|\n",
      "|   BEDFORD PARK|                   11|\n",
      "|   MOUNT BARKER|                   10|\n",
      "|     MUNNO PARA|                   10|\n",
      "|SALISBURY PLAIN|                   10|\n",
      "|         BURTON|                   10|\n",
      "|  MORPHETT VALE|                   10|\n",
      "| ELIZABETH PARK|                   10|\n",
      "|   MAWSON LAKES|                   10|\n",
      "|PARA HILLS WEST|                    9|\n",
      "+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 17.3 ms, sys: 535 µs, total: 17.9 ms\n",
      "Wall time: 5.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Importing the functions necessary\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Joining the dataframes and filtering the licence type\n",
    "join_df = units_df.join(crash_df, units_df.REPORT_ID==crash_df.REPORT_ID,how='inner')\n",
    "join_df = join_df.filter(units_df[\"Licence Type\"] == \"Unlicenced\")\n",
    "# Selecting, grouping, and aggregating required columns\n",
    "join_df = join_df.select(\"Suburb\",\"Total Cas\").groupby(\"Suburb\").agg(F.sum(\"Total Cas\").alias('Number of casualities'))\n",
    "join_df = join_df.sort(col(\"Number of casualities\").desc())\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (C) Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------+\n",
      "|         Suburb|Number of casualities|\n",
      "+---------------+---------------------+\n",
      "|       ADELAIDE|                   19|\n",
      "|      SALISBURY|                   18|\n",
      "|      DRY CREEK|                   18|\n",
      "| SALISBURY EAST|                   16|\n",
      "|       PROSPECT|                   14|\n",
      "| NORTH ADELAIDE|                   13|\n",
      "|        ENFIELD|                   12|\n",
      "|   ANDREWS FARM|                   12|\n",
      "|   BEDFORD PARK|                   11|\n",
      "|SALISBURY DOWNS|                   11|\n",
      "|     INGLE FARM|                   11|\n",
      "|SALISBURY SOUTH|                   11|\n",
      "|   MOUNT BARKER|                   10|\n",
      "|     MUNNO PARA|                   10|\n",
      "|  MORPHETT VALE|                   10|\n",
      "|SALISBURY PLAIN|                   10|\n",
      "| ELIZABETH PARK|                   10|\n",
      "|         BURTON|                   10|\n",
      "|   MAWSON LAKES|                   10|\n",
      "|ELIZABETH GROVE|                    9|\n",
      "+---------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 1.43 ms, sys: 3.78 ms, total: 5.21 ms\n",
      "Wall time: 5.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "units_df.createOrReplaceTempView(\"units_sql\") # Registering original df as a temp view\n",
    "crash_df.createOrReplaceTempView(\"crash_sql\")\n",
    "# Query to filter the suburb, format the columns\n",
    "filter_sql = spark.sql('''\n",
    "  SELECT crash_sql.Suburb,sum(crash_sql.`Total Cas`) as `Number of casualities`\n",
    "  FROM units_sql join crash_sql on units_sql.REPORT_ID == crash_sql.REPORT_ID\n",
    "  WHERE `Licence Type` = \"Unlicenced\" \n",
    "  GROUP BY Suburb\n",
    "  ORDER BY `Number of casualities` desc\n",
    "''')\n",
    "filter_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of time command, we can notice that the least wall time is observed when using dataframe, followed by SQL query, while RDD has the highest wall time. Similarly, least total time is observed in Sql query, followed by dataframe, and RDD has highest total time. From these observations, we can say that RDD are less efficient compared to SQL and Dataframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
